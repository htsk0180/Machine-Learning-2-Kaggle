{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac2a0b5",
   "metadata": {},
   "source": [
    "# Makine Öğrenmesi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b105cb0",
   "metadata": {},
   "source": [
    "**1-) Giriş**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3daa5",
   "metadata": {},
   "source": [
    "Makine öğrenimi notebook'un ikinci kısmına hoş geldiniz. Github üzerindeki birinci notebook'u çalıştıktan sonra buraya bakmanızı tavsiye ederim. Bu notebook ile modellerimizin kalitesini hızlı bir şekilde artıracağız. Gerçek veriler üzerinde işlem yapacağız ve modellerimizi doğrulamak için gelişmiş teknikleri kullanacağız. Yaygın olarak kullanılan son teknoloji modellere göz atacağız. ( XGBoost vs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e1432",
   "metadata": {},
   "source": [
    "**2-) Eksik Değerler ( Missing Values )**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d82cc",
   "metadata": {},
   "source": [
    "* Eksik değerlere sahip olan sutünları silme, \n",
    "* Imputation: Eksik değerleri bir sayı ile doldurur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c02bc",
   "metadata": {},
   "source": [
    "**Uygulama**\n",
    "Örnekte, Melbourne konut veri kümesiyle çalışacağız. Modelimiz, konut fiyatını tahmin etmek için oda sayısı ve arazi büyüklüğü gibi bilgileri kullanacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99298916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# verinin yüklenmesi.\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# hedef\n",
    "y = data.Price\n",
    "\n",
    "# sayısal değerleri kullanacağız.\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# datasetimizi train ve valid olarak ayıralım.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af695e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri temizleme yaklaşımlarının hepsini inceleyelim.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# farklı yaklaşımları karşılaştırmak adına tanımladığımız fonksiyon.\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201c0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Yaklaşımdan MAE (Eksik değerlere sahip sutünları sil.):\n",
      "354257.66157608695\n"
     ]
    }
   ],
   "source": [
    "#YÖNTEM1\n",
    "\n",
    "# eksik değerlere sahip sütunların isimlerini alalım.\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# eğitim ve doğrulama verilerinde eksik değerler içeren sutunları silelim.\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"1. Yaklaşımdan MAE (Eksik değerlere sahip sutünları sil.):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c427856",
   "metadata": {},
   "source": [
    "Eksik değerlerin tamamını sildikten sonra elde ettiğimiz değer 354257. Amacımız bu puanı düşürmek. Şimdi ise ikinci yöntemimizi deneyeceğiz. Eksik değerleri silmek yerine eksik değerin bulunduğu sütunun ortalamasını eksik değerin olduğu alana yazacağız. Bu işlem daha iyi sonuç verebilir. Ancak tabiki de veri setimize göre değişkenlik gösterebilir.İmputed değerlerini belirlemek adına daha karmaşık yollar seçilsede ( regression imputation ) gibi sonuçları makine öğrenme modellerine bağladığımızda karmaşık stratejiler genelde ek bir fayda sağlamaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc42d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Yaklaşımdan MAE (Imputation):\n",
      "203078.71828804348\n"
     ]
    }
   ],
   "source": [
    "#YÖNTEM2\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# İmputation tarafından kaldırılan sutün adlarını tekrardan yazalım.\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"2. Yaklaşımdan MAE (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff29f45",
   "metadata": {},
   "source": [
    "Yöntem 2'nin Yöntem 1'den daha düşük MAE değerine ( Mean Absolute Error  ) sahip olduğunu görüyoruz, bu nedenle Yöntem 2 bu veri kümesinde daha iyi performans gösterdi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f973ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Yaklaşımdan MAE (An Extension to Imputation):\n",
      "202839.1169021739\n"
     ]
    }
   ],
   "source": [
    "# BONUS YÖNTEM - (An Extension to Imputation)\n",
    "# Bu yöntem imputation ile eksik değerleri ifade ederken, hangi değerlerin ifade edildiğini de takip ediyoruz.\n",
    "\n",
    "# orjinal verileri değiştirmemek adına kopyalama yaptık.\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# neyin uygulanacağını gösteren yeni sütunlar oluşturduk.\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation ile silinen sutünların isimlerini tekrardan oluşturalım.\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"2. Yaklaşımdan MAE (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869d2d8",
   "metadata": {},
   "source": [
    "**3-) Categorical Variables - Kategorik Değişkenler** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680d54c",
   "metadata": {},
   "source": [
    "Kategorik bir değişken yalnızca sınırlı sayıda değerler alır. Örneğin ne sıklıkla kahvaltı yaptığınızı soran ve sadece 4 seçenek bulunan anketi düşünelim. Anket seçenekleri 'Asla', 'Nadiren', 'Çoğu Zaman' ve 'Hergün' olurdu. Bu durumda veriler kategoriktir. Çünkü yanıtlar sabit bir kategoriye girer. Ya da insanların hangi model araç kullandığı verisi de kategoriktir. Bu verileri makine öğrenmim algoritmalarına direkt olarak verirsek hata alırız. Bu bölümde kategorik verileri makine öğrenim algoritmalarına hazırlamak için 3 temel adıma göz atacağız.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09040104",
   "metadata": {},
   "source": [
    "* *1) Kategorik Verileri Silin.*\n",
    "* *2) Sıralı Encoding:* Bu işlem her benzersiz değeri bir tamsayıya atar. Makine öğrenim algoritmalarından ağaç tabanlı modeller için ( karar ağaçları , rastgele ormanlar ) sıralı kodlamanın sıralı değişkenlerde iyi çalıştığı gözlemlenmiştir.\n",
    "* *3) One-Hot Encoding:* Tek satırlı kodlama, orijinal verilerdeki olası her değerin varlığını (veya yokluğunu) gösteren yeni sütunlar oluşturur. Mantığı şudur: Benzersiz kolon değerleri ile matris oluşturulur. Satır ve sutunu aynı olan değerler 1, diğerleri 0 değerini alır.Sıralı kodlamanın aksine, tek satırlı kodlama kategorilerin sırasını varsaymaz. Bu nedenle, kategorik verilerde net bir sıralama yoksa bu yaklaşımın özellikle iyi çalışmasını bekleyebilirsiniz. İçsel sıralaması olmayan kategorik değişkenleri *nominal değişkenler* olarak adlandırırız.Kategorik değişken çok sayıda değer alırsa (yani, genellikle 15'ten fazla farklı değer alan değişkenler için), tek satırlı kodlama genellikle iyi performans göstermez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579499e",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d525be0",
   "metadata": {},
   "source": [
    "Önceki eğitimde olduğu gibi, Melbourne Konut veri seti ile çalışacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3f96a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datanın yüklenmesi.\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# görev dağılımı.\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# datasetin eğitim/doğrulama olarak ayrılması.\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# kayıp değerlerin silinmesi.\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" bir sutündaki benzersiz değerler anlamına gelir.\n",
    "# benzersiz değerlere sahip sutünların seçimi.\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# sayısal olan sütunların seçimi.\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# yalnızca seçili olan sütunları sakla. çünkü kategorik veriler ile işlem yapacağız.\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8185cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>h</td>\n",
       "      <td>SP</td>\n",
       "      <td>3349</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>h</td>\n",
       "      <td>SP</td>\n",
       "      <td>2686</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>6065</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8832</th>\n",
       "      <td>h</td>\n",
       "      <td>VB</td>\n",
       "      <td>11346</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>13474</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method  Unnamed: 0  Rooms\n",
       "2573     h     SP        3349      4\n",
       "2091     h     SP        2686      3\n",
       "4683     u      S        6065      2\n",
       "8832     h     VB       11346      3\n",
       "10469    u      S       13474      2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head kullanarak eğitim verilerine şöyle bir göz atalım.\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62628caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kategorik değişkenler:\n",
      "['Type', 'Method']\n"
     ]
    }
   ],
   "source": [
    "# şimdi ise eğitim verilerindeki tüm kategorik verilerin listesini alalım. Bunu her sütunun veri tipini kontrol ederek yaparız.\n",
    "# nesne türü bir sütunun metin olduğunu gösterir. bu verisetimizde metin içeren sütünlar bizim için kategorik değişkendir.\n",
    "\n",
    "# eğitim setimizdeki kategorik değişkenleri liste şeklinde alalım.\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"kategorik değişkenler:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08a417af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bir önceki örnekte yaptığımız gibi yöntemlerimizi denemek adına fonksiyonumuzu tanımlıyoruz.\n",
    "# aynı şekilde bu fonksiyon aracılığı ile MAE hesabını yapacağız ve en düşük hata değerini yakalamaya çalışacağız.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# karşılaştırma yapabilmemiz adına yöntemlerin skolarlarını elde edeceğimiz fonksiyonumuz.\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4bd390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. yöntem için MAE değeri: (kategorik değerlerin silinmesi yöntemi)\n",
      "345278.21768750006\n"
     ]
    }
   ],
   "source": [
    "# YÖNTEM 1: select_type ile tipi object yani string yani kategorik değer içeren bütün sutunları siliyoruz.\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"1. yöntem için MAE değeri: (kategorik değerlerin silinmesi yöntemi)\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c1b99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. yöntem için MAE değeri: (Ordinal Encoding yöntemi)\n",
      "314139.64080978255\n"
     ]
    }
   ],
   "source": [
    "# YÖNTEM 2: Ordinal Encoding - Sıralı Kodlama- Bu işlem için sklearn'in OrdinalEncoder paketini import edeceğiz.\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# orjinal datamız bozulmasın diye kopyalayalım. \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# kategorik veriler içeren her sütuna ordinal encoder uygulayalım.\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"2. yöntem için MAE değeri: (Ordinal Encoding yöntemi)\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa402135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. yöntem için MAE değeri: (OneHotEncoding yöntemi)\n",
      "314342.2874402174\n"
     ]
    }
   ],
   "source": [
    "# YÖNTEM 3: One-Hot Encoding - Bu işlem için sklearn'in OneHotEncoder  paketini import edeceğiz.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# kategorik verilerin her sutünuna onehotencoding uyguluyoruz.\n",
    "\n",
    "# handle_unknown='ignore' parametresini ile doğrulama verileri eğitim verilerinde temsil edilmeyen sınıflar içerdiğinde\n",
    "# hatalardan kaçınmak için belirledik\n",
    "\n",
    "# sparse=False parametresi, kodlanmış sütunların numpy dizisi olarak döndürülmesini sağlar.\n",
    "\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# onehot ile kaldırılan sutün isimlerini geri yazalım.\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# kategorik sütunlar kaldırılır.\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# diğer sayısal özelliklere onehot ile kodlanmış sutünları ekleyelim.\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"3. yöntem için MAE değeri: (OneHotEncoding yöntemi)\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3b713",
   "metadata": {},
   "source": [
    "Bu dataset için en iyi yöntem , Yöntem 2 oldu. Tabi bu durum datasetler için değişebilir. Dünyamız kategorik veriler ile doludur. Artık kategorik veriler için nasıl çalışacağımızı biliyoruz. İyi bir data science olma yolunda hızla gidiyorsun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732d62a",
   "metadata": {},
   "source": [
    "**4-) Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76416ec7",
   "metadata": {},
   "source": [
    "Pipelines veri ön işleme ve modelimizi oluşturduğumuz kodun düzenli kalmasına yardımcı olan en basit tekniklerden bir tanesidir. Veri ön işleme ve modelleme adımlarının bir araya geldiği yerdir. Böylece bu iki adımı da tek bir adımmış gibi kullanabiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264539d1",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c6b29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datanın yüklenmesi.\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# görevlendirme.\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# train ve doğrulama olarak verisetinin ayarlanması.\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" bir sutündaki benzersiz değerler anlamına gelir.\n",
    "# benzersiz değerlere sahip sutünların seçimi.\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# sayısal değerlerin seçimi.\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# # yalnızca seçili olan sütunları sakla. çünkü kategorik veriler ile işlem yapacağız.\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8334454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>h</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3349</td>\n",
       "      <td>4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3058.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>-37.7337</td>\n",
       "      <td>144.9548</td>\n",
       "      <td>11204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>h</td>\n",
       "      <td>SP</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>2686</td>\n",
       "      <td>3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3124.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>-37.8436</td>\n",
       "      <td>145.0581</td>\n",
       "      <td>8920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>6065</td>\n",
       "      <td>2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.8126</td>\n",
       "      <td>145.0534</td>\n",
       "      <td>10331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8832</th>\n",
       "      <td>h</td>\n",
       "      <td>VB</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>11346</td>\n",
       "      <td>3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3123.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.8396</td>\n",
       "      <td>145.0514</td>\n",
       "      <td>6482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>13474</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3181.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2842.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>-37.8513</td>\n",
       "      <td>144.9943</td>\n",
       "      <td>7717.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Unnamed: 0  Rooms  Distance  \\\n",
       "2573     h     SP  Northern Metropolitan        3349      4       7.8   \n",
       "2091     h     SP  Southern Metropolitan        2686      3       7.8   \n",
       "4683     u      S  Southern Metropolitan        6065      2       5.6   \n",
       "8832     h     VB  Southern Metropolitan       11346      3       7.5   \n",
       "10469    u      S  Southern Metropolitan       13474      2       4.5   \n",
       "\n",
       "       Postcode  Bedroom2  Bathroom  Car  Landsize  BuildingArea  YearBuilt  \\\n",
       "2573     3058.0       4.0       2.0  1.0     381.0           NaN     1938.0   \n",
       "2091     3124.0       3.0       1.0  1.0     544.0         160.0     1930.0   \n",
       "4683     3101.0       2.0       1.0  1.0     121.0           NaN        NaN   \n",
       "8832     3123.0       3.0       2.0  2.0     200.0           NaN        NaN   \n",
       "10469    3181.0       2.0       1.0  1.0    2842.0          84.0     1920.0   \n",
       "\n",
       "       Lattitude  Longtitude  Propertycount  \n",
       "2573    -37.7337    144.9548        11204.0  \n",
       "2091    -37.8436    145.0581         8920.0  \n",
       "4683    -37.8126    145.0534        10331.0  \n",
       "8832    -37.8396    145.0514         6482.0  \n",
       "10469   -37.8513    144.9943         7717.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head() # veriler hem kategorik hemde eksik değerler içeriyor. pipelines ile bunların hakkından gelmek kolaydır. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84ff41",
   "metadata": {},
   "source": [
    "Pipelines'ı 3 ana adımda inşa ediyoruz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884a946",
   "metadata": {},
   "source": [
    "* *ADIM:1 ÖN İŞLEME ADIMLARINI TANIMLAYIN:*\n",
    "* Farklı ön işleme adımlarını bir araya getirmek için Column Transforme sınıfını kullanırız.\n",
    "* Aşağıda yazdığımız kod bloğu sayısal verilerdeki eksik değerleri ifade eder ve kategorik verilere onehot enconding i uygular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4924836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# sayısal veriler için ön işleme.\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# kategorik veriler için ön işleme.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# sayısal ve kategorik veriler için paket ön işleme.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c35d00",
   "metadata": {},
   "source": [
    "* *ADIM:2 MODELİ TANIMLAYIN:*\n",
    "* RandomForestRegressor sınıfıyla rastgele bir orman modeli ( random forest ) tanımlarız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "283f8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9f6b",
   "metadata": {},
   "source": [
    "* *PIPELINE'I OLUŞTURMA VE DEĞERLENDİRME:*\n",
    "* Son olarak ön işleme ve modelleme adımlarını birleştiren pipeline i tanımlamak adına Pipeline class'ını kullanacağız. Dikkat edilmesi gereken bir kaç önemli nokta var.\n",
    "    *Pipeline ile eğitim verilerini önceden işleriz.Modeli tek bir kod satırına sığdırırız. ( Bu karşılık eğer pipeline kullanmasaydık imputation, onehotencoding ve model eğitimi ayrı adımlar ile yapılması gerekecekti. Hem kategorik hemde sayısal veriler ile uğraşmak yazdığımız kodu bir hayli karışık hale getirecekti. ) \n",
    "    *Pipeline ile X_valid içindeki işlenmemiş özellikleri predict() komutuna veririz. Böylece pipeline tahminler oluşturulmadan önce özellikleri otomatik olarak önceden işler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b3e8a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 177453.58753804347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# paket ön işleme ve modelleme kodumuzu buraya aldık.\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# eğtim verilerini ön işlemeden geçirip akabinde eğitiyoruz.\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# doğrulama verilerini ön işlemeden geçirip akabinde tahminler üretiyoruz.\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# modelimizi değerliyoruz.\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f44eca",
   "metadata": {},
   "source": [
    "**5-) Cross-Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b924ca",
   "metadata": {},
   "source": [
    "Bu bölümde modelimizin performansını daha iyi ölçmek adına çapraz doğrulamayı (cross validation) öğreneceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9f983",
   "metadata": {},
   "source": [
    "Makine öğrenimi yinelemeli bir süreçtir. Doğrulama kümemiz ne kadar büyükse modelin kalitesini ölçtüğümüzde o kadar az rastgelelik vardır ve bir o kadar da güvenilir doğrulamalar alınacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ab787",
   "metadata": {},
   "source": [
    "Çağraz doğrulamada birden fazla model kalitesi ölçümü elde etmek için modelleme sürecimizi datasetimizin farklı alt kümeleri üzerinde çalıştırırız."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c032d2",
   "metadata": {},
   "source": [
    "Örneğin datasetimizi 5 adet eşit parçaya böldüğümüzü varsayalım.( 5 folds ). Ardından her kat için bir deney yaparız. *Deney 1* de ilk fold'u doğrulama kalan 4 parçayı ise eğitim olarak kullanırız. Sonra *Deney 2* de 2. foldu doğrulama sonra kalan diğer 4 fold'u ise eğitim şeklinde bütün fold'lara test/validation şansı veririz. Bu şekilde verilerin tamamını hem eğitim de hem de validation da kullanmış oluruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382a0d4",
   "metadata": {},
   "source": [
    "* Peki cross validation'u ne zaman kullanmalıyız ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f737a3e",
   "metadata": {},
   "source": [
    "Çapraz doğrulama model kalitesi hakkında daha doğru bir ölçü verir. Bununla birlikte derleme süresi artar. Ekstra hesaplama gücünün önemli olmadığı küçük veri kümelerinde çapraz doğrulama uygulanmaldır. Daha büyük veri kümeleri için tek bir doğrulama kümesi yeterlidir. Alternatif olarak, çapraz doğrulamayı çalıştırabilir ve her denemenin puanlarının yakın görünüp görünmediğini kontrol edebilirsiniz. Her deneme aynı sonuçları verirse, tek bir doğrulama kümesi muhtemelen yeterlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2945ea5",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d19594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# datasetin yüklenmesi.\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# alt küme seçimi.\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# görev atama \n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0be7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri ön işleme ve modelimizi oluşturmak adına pipeline tanımını yapıyoruz.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300278d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE skoru:\n",
      " [330153.84614038 315703.21408613 299410.61884391 243541.74763508\n",
      " 251301.76683685]\n"
     ]
    }
   ],
   "source": [
    "# çapraz doğrulama skorlarınu sklearn'ın cross_val_score() fonksiyonu ile elde ederiz. \n",
    "# fold sayısını ise cv parametresi ile belirliyoruz.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# sklearn negatif MAE yi hesapladığından dolayı sonucu -1 ile çarptık.\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE skoru:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b369b3",
   "metadata": {},
   "source": [
    "Alternatif modelleri karşılaştırmak için genellikle tek bir model kalitesi ölçüsü istiyoruz. Bu yüzden deneylerin ortalamasını alıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfee7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama MAE skoru:\n",
      "288022.23870846874\n"
     ]
    }
   ],
   "source": [
    "print(\"Ortalama MAE skoru:\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f396147",
   "metadata": {},
   "source": [
    "**6-) XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d0ee1",
   "metadata": {},
   "source": [
    "Bu bölümde gradient boosting ile modellerimizi nasıl inşa edeceğimizi ve nasıl optimize edeceğimizi öğreneceğiz. Bu yöntem çeşitli veri kümelerinde son teknolojiyi içeren sonuçlar elde eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1650aa4",
   "metadata": {},
   "source": [
    "Bu dersten önce genelde makine öğrenim algoritması olarak random forest algoritmasını kullandık. Random forest yöntemlerini **'ensemble method'** olarak adlandırıyoruz. Ensemble method çeşitli modellerin tahminlerini birleştirir.Gradient boosting de  ensemble method grubuna girer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f71f5",
   "metadata": {},
   "source": [
    "**Gradient Boosting:** modelleri yinelemeli olarak döngülerden geçiren yöntemdir. Yöntem basit bir tahmin yapan model ile başlar. Veri kümesindeki her gözlem için tahminleme yapmak için geçerli topluluğu kullanırız. Bir tahmin yapmak için topluluktaki tüm modellerin tahminlerini ekleriz. Bu tahminler bir kayıp fonksiyonu hesaplar.( örneğin MAE) Ardından topluluğa eklenecek yeni model için kayıp işlevini kullanırız. Özellikle model parametrelerini belirliyoruz. Böylece yeni modeli topluluğa eklemek genel kaybı azaltacaktır. Bu işlem en az loss değeri elde edilene kadar tekrar edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c756dad",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6355e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# verisetinin yüklenmesi.\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# tahmin için kullanılacak alt kümelerin seçimi.\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# görevlendirme.\n",
    "y = data.Price\n",
    "\n",
    "# eğitim ve test olarak verilerin ayrılması.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6afab1e",
   "metadata": {},
   "source": [
    "Bu uygulamada XGBoost ile çalışacağız. XGBoost performans ve hıza odaklanan birkaç ek özelliğe sahip gradient boosting'in extreme edilmiş halidir.( sklearn gradient boosting'in özelliğine sahip farklı sürümlerine sahiptir ancak XGBoost'un bazı teknik avantajları vardır. Aşağıdaki kod bloğunda XGBoost için sklearn API'sini içeri aktarıyoruz(xgboost.XGBRegressor). Bu, scikit-learn'de yaptığımız gibi bir model oluşturmamızı ve tahminleme yapmamıza yardımcı olacaktır.(XGBRegressor sınıfının birçok ayarlanabilir parametresi vardır)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f61e9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# modelin oluşturulması:\n",
    "my_model = XGBRegressor()\n",
    "\n",
    "# eğitim:\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f68456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 247486.19087709286\n"
     ]
    }
   ],
   "source": [
    "# tahminleme:\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc6d7f",
   "metadata": {},
   "source": [
    "**Parameter Tuning - Parametre Ayarı**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84231ac",
   "metadata": {},
   "source": [
    "XGBoost'un doğruluğu ve hızını önemli ölçüde etkileyecek çok önemli parametreler mevcuttur. Bu parametreleri anlamak çok çok önemlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c66773",
   "metadata": {},
   "source": [
    "* 1-) **n_estimators** : yukarıda açıklanan modelleme döngüsünden kaç kaç yapılacağıdır. topluluğa dahil ettiğimiz model sayısına eşittir. az ya da fazla verilmesi underfitting ve overfittinge neden olurken tipik olarak 10-1000 arasında değişir. ancak bu değer learning rate parametresine birebir bağlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a41a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=500, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88a178",
   "metadata": {},
   "source": [
    "* 2-) **early_stopping_rounds** : n_estimators için ideal değeri otomatik olarak bulmanın bir yolunu sunar. erken durdurma n_estimators değerine gelmeden validation skor iyileştirmeyi durdurduğunda modelin yinelemeyi durdurmasına neden olur. durdurma gerçekleşmeden önce kaç kez bozulmaya izin verileceği ayarlanır. early_stopping_rounds=5 ideal bir parametredir. bu durumda kötüleşen validation skor üst üste 5 kez meydana geldiğinde yineleme durur. early_stoppıng_rounds kullanırken, doğrulama puanlarını hesaplamak için bazı verileri de ayırmanız gerekir. bu, eval_set parametresini ayarlayarak yapılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6a043e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=500, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=500)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)],\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db4973",
   "metadata": {},
   "source": [
    "Daha sonra tüm verilerinizle bir modele eğitmek isterseniz, n_estimators öğesini erken durdurma ile çalıştırıldığında en uygun bulduğunuz değere ayarlayın."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9dba6",
   "metadata": {},
   "source": [
    "* 3-) **learning_rate**: Genel olarak, küçük bir öğrenme oranı ve çok sayıda tahminci daha doğru XGBoost modelleri verecektir, ancak döngü boyunca daha fazla yineleme yaptığı için modelin eğitilmesi daha uzun sürecektir. Varsayılan olarak, XGBoost learning_rate=0.1 değerini ayarlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fce07db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=1000, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18180e0b",
   "metadata": {},
   "source": [
    "* 4-) **n_jobs**: Çalışma zamanının dikkate alındığı daha büyük veri kümelerinde, modellerinizi daha hızlı oluşturmak için paralellik kullanabilirsiniz. n_jobs parametresini makinenizdeki çekirdek sayısına eşit olarak ayarlamak yaygın kullanımıdır. Daha küçük veri kümelerinde genellikle bu parametre kullanılmaz. Ortaya çıkan model daha iyi olmayacak, ancak, fit() komutu sırasında uzun süre bekleyeceğiniz büyük veri kümelerinde kullanışlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe28bd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=1000, n_jobs=4,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628c42e",
   "metadata": {},
   "source": [
    "XGBoost, standart tablo verileriyle ( pandas dataframe'inde sakladığınız veri türü) çalışmak için önde gelen bir yazılım kütüphanesidir. Dikkatli parametre ayarlaması ile son derece hassas modelleri eğitebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f7734",
   "metadata": {},
   "source": [
    "**7-) Data Leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b974ba6",
   "metadata": {},
   "source": [
    "Bu bölümde modelimizi bozan küçük verileri bulup düzeltmeyi öğreneceğiz. Eğitim verilerimiz hedef hakkında bilgi içerdiğinde veri sızıntısı ( Data leakage (veya leakage)) meydana gelir. Bu durum eğitim hatta doğrulama setinde yüksek performansa yol açarken model gerçek hayatta kötü performans gösterecektir. Başka bir deyişle modelle karar vermeye başlayana kadar modelin doğru görünmesine neden olurken aslında model çok kötüdür. İki ana Data leakage türü vardır. *target leakage* ve *train-test contamination.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d021673",
   "metadata": {},
   "source": [
    "* Target leakage: tahminde bulunduğumuz esnada tahminlerimiz kullanılamayan verileri içerdiğinde oluşur. yani eğitim verilerinde biri değiştiğinde sonuç önemli ölçüde yanlış olarak değişiyorsa meydana gelir.Bu tür veri sızıntısını önlemek için, hedef değer gerçekleştirildikten sonra güncellenen (veya oluşturulan) herhangi bir değişken hariç tutulmalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb734915",
   "metadata": {},
   "source": [
    "* Train-Test Contamination: Eğitim verilerini doğrulama verilerinden ayırmaya dikkat etmediğinizde farklı bir sızıntı türü oluşur. Doğrulama verileri ön işleme davranışını etkiliyorsa, bu işlemi ince yollarla bozabilirsiniz. Buna train-test contamination denir.Örneğin, train_test_split() öğesini çağırmadan önce ön işlemeyi (eksik değerler için bir imputer fit etmek gibi) çalıştırdığınızı düşünün. Sonuç? Modeliniz iyi doğrulama puanları alabilir, bu da size büyük bir güven verebilir, ancak karar vermek için dağıttığınızda kötü performans gösterebilir.Doğrulamanız basit bir train/validation bölünmesine dayanıyorsa, doğrulama verilerini ön işleme adımlarının birleştirilmesi de dahil olmak üzere her türlü birleştirmeden hariç tutun. Scikit-learn pipeline'si kullanırsanız bu daha kolaydır. Çapraz doğrulama kullanırken, ön işleminizi pipeline içinde yapmamız daha da kritik!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384679a5",
   "metadata": {},
   "source": [
    "**Uygulama**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698e444",
   "metadata": {},
   "source": [
    "Kredi kartı uygulamaları hakkında bir veri kümesi kullanacağız. Sonuç olarak, her bir kredi kartı başvurusu hakkındaki bilgiler bir x dataframe'inde saklanır. Bunu, bir seride hangi başvuruların kabul edildiğini tahmin etmek için kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aff65be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verisetimizdeki satır sayısı: 1319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reports</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>share</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>owner</th>\n",
       "      <th>selfemp</th>\n",
       "      <th>dependents</th>\n",
       "      <th>months</th>\n",
       "      <th>majorcards</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37.66667</td>\n",
       "      <td>4.5200</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>124.983300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>33.25000</td>\n",
       "      <td>2.4200</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>9.854167</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>33.66667</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>30.50000</td>\n",
       "      <td>2.5400</td>\n",
       "      <td>0.065214</td>\n",
       "      <td>137.869200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>32.16667</td>\n",
       "      <td>9.7867</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>546.503300</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reports       age  income     share  expenditure  owner  selfemp  \\\n",
       "0        0  37.66667  4.5200  0.033270   124.983300   True    False   \n",
       "1        0  33.25000  2.4200  0.005217     9.854167  False    False   \n",
       "2        0  33.66667  4.5000  0.004156    15.000000   True    False   \n",
       "3        0  30.50000  2.5400  0.065214   137.869200  False    False   \n",
       "4        0  32.16667  9.7867  0.067051   546.503300   True    False   \n",
       "\n",
       "   dependents  months  majorcards  active  \n",
       "0           3      54           1      12  \n",
       "1           3      34           1      13  \n",
       "2           4      58           1       5  \n",
       "3           0      25           1       7  \n",
       "4           2      64           1       5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# datanın yüklenmesi\n",
    "data = pd.read_csv('AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'], false_values = ['no'])\n",
    "\n",
    "# görev\n",
    "y = data.card\n",
    "\n",
    "# tahmin\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "print(\"verisetimizdeki satır sayısı:\", X.shape[0])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c722c2",
   "metadata": {},
   "source": [
    "Bu küçük bir veri kümesi olduğundan, model kalitesinin doğru ölçümlerini sağlamak için çapraz doğrulama kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8d66b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy: 0.980292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# ön işleme olmadığından bir pipeline a ihtiyacımız yok. ancak yine de yazalım elimiz alışşın :)\n",
    "my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\n",
    "cv_scores = cross_val_score(my_pipeline, X, y, \n",
    "                            cv=5,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation accuracy: %f\" % cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf7147",
   "metadata": {},
   "source": [
    "Verisetimizi incelemek çok önemlidir. Bazı veriler tehlikelidir. Veri sızıntısına neden olabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41db0d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kart alamayan ve harcaması olmayanların oranı: 1.00\n",
      "Kart alan ve harcaması olmayanların oranı: 0.02\n"
     ]
    }
   ],
   "source": [
    "expenditures_cardholders = X.expenditure[y]\n",
    "expenditures_noncardholders = X.expenditure[~y]\n",
    "\n",
    "print('Kart alamayan ve harcaması olmayanların oranı: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))\n",
    "print('Kart alan ve harcaması olmayanların oranı: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe35ab",
   "metadata": {},
   "source": [
    "Yukarıda gösterildiği gibi, kart almayan herkesin harcaması yokken, kart alanların sadece % 2'sinin harcaması yoktu.Ancak harcamaların muhtemelen başvurdukları karttaki harcamalar anlamına geldiği bir hedef sızıntısı durumu gibi görünüyor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a1f4454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.831683\n"
     ]
    }
   ],
   "source": [
    "# Veri kümesinden veri sızdıran tahmin etmek için eğiteceğimiz başlıkları silelim.\n",
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "\n",
    "#bu şekilde modelimizi değerleyelim.\n",
    "cv_scores = cross_val_score(my_pipeline, X2, y, \n",
    "                            cv=5,\n",
    "                            scoring='accuracy')\n",
    "\n",
    "print(\"Cross-val accuracy: %f\" % cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02426b",
   "metadata": {},
   "source": [
    "Evet çapraz doğrulamada değerimiz düştü. Bu istemediğimizi bir durumdu. Ancak veri sızdıran değerleri çıkartmadan önce aldığımız 0.980292 sonuç iyi gibi görünebilir. Ancak veri sızdırma durumu olduğundan gerçek problemlerde iyi çalışmayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0b6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
